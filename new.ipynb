{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_smsspam_dataset(df_data):\n",
    "    # covert uppercase letters to lowercase letters\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        x.lower() for x in x.split()\n",
    "    ))\n",
    "\n",
    "    # delete puctuation marks\n",
    "    df_data['text'] = df_data['text'].str.replace('[^\\w\\s]', '')\n",
    "\n",
    "    # delete numbers from texts\n",
    "    df_data['text'] = df_data['text'].str.replace('\\d', '')\n",
    "\n",
    "    # delete stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop_words.update(punctuation)\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        x for x in x.split() if x not in stop_words\n",
    "    ))\n",
    "\n",
    "    # lemmatization and get the roots of the words\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        [Word(word).lemmatize() for word in x.split()]\n",
    "    ))\n",
    "\n",
    "    # remove words less than 3 letters\n",
    "    df_data['text'] = df_data['text'].apply(lambda x: ' '.join(\n",
    "        [x for x in x.split() if len(x) > 3]\n",
    "    ))\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def preprocessing_smsspam_dataset(datafile):\n",
    "    # load dataset\n",
    "    df = pd.read_csv(datafile, encoding='ISO-8859-1', \n",
    "                   engine='python')\n",
    "    #rename dataset columns\n",
    "    df.rename(columns = {\"v1\": \"target\", \"v2\": \"text\"}, inplace = True)\n",
    "\n",
    "    #drop unnecessary columns\n",
    "    df.drop([\"Unnamed: 2\",\"Unnamed: 3\", \"Unnamed: 4\"], axis = 1, inplace = True)\n",
    "\n",
    "    # drop duplicate data\n",
    "    df.drop_duplicates(inplace = True)\n",
    "\n",
    "    # cleaning data\n",
    "    df = cleaning_smsspam_dataset(df)\n",
    "\n",
    "    df['target'].replace({'ham': 0, 'spam': 1}, inplace=True)\n",
    "\n",
    "    print(\"+++ httpparams dataset: +++\")\n",
    "    print(\"\\tNumber of normal requests: \", len(df[df['target'] == 0]))\n",
    "    print(\"\\tNumber of anomalous requests: \", len(df[df['target'] == 1]))\n",
    "    print(\"\\tNumber of total requests: \", df.shape[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smsspam_load_data(test_prob=0.2, max_len=71):\n",
    "    data_file = './datasets/smsspam/spam.csv'\n",
    "    data = preprocessing_smsspam_dataset(data_file)\n",
    "\n",
    "    x_data = data['text'].values\n",
    "    y_data = data['target'].values\n",
    "\n",
    "    tokenizer = get_tokenizer(tokenizer='spacy', language='en_core_web_sm')\n",
    "    \n",
    "    # print(x_data)\n",
    "    vocab = build_vocab_from_iterator([tokenizer(text) for text in x_data])\n",
    "\n",
    "    print(vocab.__len__())\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    \n",
    "    temp_x_data = [text_pipeline(text) for text in x_data]\n",
    "    temp_x_data = [np.asarray(sample, dtype=np.int32) for sample in temp_x_data]\n",
    "    temp_x_data = [np.pad(sample, (0, max(0, max_len-len(sample))), mode='constant', constant_values=0) for sample in temp_x_data]\n",
    "    temp_x_data = [sample[:max_len] for sample in temp_x_data]\n",
    "    x_data = np.asarray(temp_x_data, dtype=np.int32)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_prob,\n",
    "                                                        shuffle=True, random_state=120124)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4232\\2753061372.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_data['text'] = df_data['text'].str.replace('[^\\w\\s]', '')\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4232\\2753061372.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_data['text'] = df_data['text'].str.replace('\\d', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ httpparams dataset: +++\n",
      "\tNumber of normal requests:  4516\n",
      "\tNumber of anomalous requests:  653\n",
      "\tNumber of total requests:  5169\n",
      "6972\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "x_train, y_train, x_test, y_test = smsspam_load_data()\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "data_loader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define BIDIRECT LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=6972, embed_dim=128, hidden_dim=32, nb_classes=2, n_layers=2):\n",
    "        super(LSTMNet,self).__init__()\n",
    "        \n",
    "        self.emded_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.emded_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer process the vector sequences \n",
    "        self.lstm_1 = nn.LSTM(input_size=self.emded_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=True,\n",
    "                            dropout=0.2,\n",
    "                            batch_first=True\n",
    "                           )\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.lstm_2 = nn.LSTM(input_size=self.hidden_dim*2,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True\n",
    "                           )\n",
    "\n",
    "        self.fc = nn.Linear(in_features=self.hidden_dim*2, out_features=nb_classes) # 2 for bidirection\n",
    "        # Prediction activation function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "        embedded = self.embedding(x_batch)\n",
    "        # print(embedded.shape)\n",
    "        hidden_0 = torch.randn(2*self.n_layers, len(x_batch), self.hidden_dim).to(device) # 2 for bidirection\n",
    "        carry_0 = torch.randn(2*self.n_layers, len(x_batch), self.hidden_dim).to(device)\n",
    "        # print(hidden_0.shape)\n",
    "        output, (hidden_1, carry_1) = self.lstm_1(embedded, (hidden_0, carry_0))\n",
    "        # print(output.shape)\n",
    "        # print(hidden_1.shape)\n",
    "        output = self.tanh(output)\n",
    "        output, (hidden_2, carry_2) = self.lstm_2(output, (hidden_1, carry_1))\n",
    "        #Final activation function\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        # print(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546690\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample_data = torch.randint(0, 6972, (32, 71)).to(device)\n",
    "model = LSTMNet().to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(pytorch_total_params)\n",
    "output = model(sample_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tek4fed.decorator import timer\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import gc\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "model = LSTMNet()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "@timer\n",
    "def train():\n",
    "    model.to(device)\n",
    "    # set the model_lib in training mode\n",
    "    model.train()\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    test_correct = 0\n",
    "    for (x_batch, y_batch) in data_loader:\n",
    "        # send the input to the device\n",
    "        (x_batch, y_batch) = (x_batch.to(device),\n",
    "                                y_batch.long().to(device))\n",
    "        \n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        # zero out the gradients, perform the backpropagation step,\n",
    "        # and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_test_loss = total_test_loss + loss\n",
    "        test_correct = test_correct + (pred.argmax(1) == y_batch).type(\n",
    "            torch.float\n",
    "        ).sum().item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(data_loader)\n",
    "    test_correct = test_correct / len(x_train)\n",
    "\n",
    "    results_dict = {\n",
    "        'loss': avg_test_loss.cpu().detach().item(),\n",
    "        'accuracy': test_correct\n",
    "    }\n",
    "\n",
    "\n",
    "    print(results_dict)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "{'loss': 0.49329206347465515, 'accuracy': 0.8727932285368802}\n",
      "\t\t Total time taken to train: 1.0474s\n",
      "Epoch: 1\n",
      "{'loss': 0.4405587315559387, 'accuracy': 0.8727932285368802}\n",
      "\t\t Total time taken to train: 0.8752s\n",
      "Epoch: 2\n",
      "{'loss': 0.44076070189476013, 'accuracy': 0.8727932285368802}\n",
      "\t\t Total time taken to train: 0.8041s\n",
      "Epoch: 3\n",
      "{'loss': 0.44086816906929016, 'accuracy': 0.8727932285368802}\n",
      "\t\t Total time taken to train: 0.8158s\n",
      "Epoch: 4\n",
      "{'loss': 0.44052964448928833, 'accuracy': 0.8727932285368802}\n",
      "\t\t Total time taken to train: 0.7930s\n",
      "Epoch: 5\n",
      "{'loss': 0.44065067172050476, 'accuracy': 0.8727932285368802}\n",
      "\t\t Total time taken to train: 0.7886s\n",
      "Epoch: 6\n",
      "{'loss': 0.44037896394729614, 'accuracy': 0.8727932285368802}\n",
      "\t\t Total time taken to train: 0.7974s\n",
      "Epoch: 7\n",
      "{'loss': 0.3937194347381592, 'accuracy': 0.9172914147521161}\n",
      "\t\t Total time taken to train: 0.7811s\n",
      "Epoch: 8\n",
      "{'loss': 0.3393701910972595, 'accuracy': 0.973881499395405}\n",
      "\t\t Total time taken to train: 0.7876s\n",
      "Epoch: 9\n",
      "{'loss': 0.3311326205730438, 'accuracy': 0.9816203143893591}\n",
      "\t\t Total time taken to train: 0.8054s\n",
      "Epoch: 10\n",
      "{'loss': 0.32757696509361267, 'accuracy': 0.9862152357920193}\n",
      "\t\t Total time taken to train: 0.7856s\n",
      "Epoch: 11\n",
      "{'loss': 0.325457364320755, 'accuracy': 0.9881499395405079}\n",
      "\t\t Total time taken to train: 0.7896s\n",
      "Epoch: 12\n",
      "{'loss': 0.32402217388153076, 'accuracy': 0.9893591293833132}\n",
      "\t\t Total time taken to train: 0.7925s\n",
      "Epoch: 13\n",
      "{'loss': 0.3237082362174988, 'accuracy': 0.9896009673518742}\n",
      "\t\t Total time taken to train: 0.7830s\n",
      "Epoch: 14\n",
      "{'loss': 0.3231102526187897, 'accuracy': 0.9903264812575574}\n",
      "\t\t Total time taken to train: 0.7796s\n",
      "Epoch: 15\n",
      "{'loss': 0.322703093290329, 'accuracy': 0.9908101571946796}\n",
      "\t\t Total time taken to train: 0.8051s\n",
      "Epoch: 16\n",
      "{'loss': 0.32226309180259705, 'accuracy': 0.9910519951632406}\n",
      "\t\t Total time taken to train: 0.8403s\n",
      "Epoch: 17\n",
      "{'loss': 0.3222863972187042, 'accuracy': 0.9910519951632406}\n",
      "\t\t Total time taken to train: 0.7920s\n",
      "Epoch: 18\n",
      "{'loss': 0.3220241367816925, 'accuracy': 0.9912938331318016}\n",
      "\t\t Total time taken to train: 0.8275s\n",
      "Epoch: 19\n",
      "{'loss': 0.3221302628517151, 'accuracy': 0.9912938331318016}\n",
      "\t\t Total time taken to train: 0.8120s\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    print('Epoch: {}'.format(e))\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLDP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "feb25b4c7695d3b176bda5de41a7795a3d4786c47c38e1dae1d59a33add862d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
